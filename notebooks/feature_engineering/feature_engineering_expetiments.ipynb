{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8d87b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios para todos los experimentos de Feature Engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, FastICA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "# Diccionario global para almacenar resultados de todos los experimentos\n",
    "global_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f0e3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../datasets/transaction_dataset_clean.csv\")\n",
    "X = df.drop(columns=[\"FLAG\"])\n",
    "y = df[\"FLAG\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a782b8",
   "metadata": {},
   "source": [
    "### Experimento Baseline: Evaluaci√≥n con caracter√≠sticas originales\n",
    "    \n",
    "Este experimento establece la l√≠nea base usando las caracter√≠sticas originales sin ninguna transformaci√≥n o ingenier√≠a de features. Sirve como punto de referencia para comparar todos los dem√°s experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92653503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_baseline(X_train, X_test, y_train, y_test):\n",
    "    \"\"\" \n",
    "    Par√°metros:\n",
    "    - X_train: DataFrame con caracter√≠sticas de entrenamiento\n",
    "    - X_test: DataFrame con caracter√≠sticas de test\n",
    "    - y_train: Series con etiquetas de entrenamiento\n",
    "    - y_test: Series con etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con resultados del experimento baseline\n",
    "    \"\"\"\n",
    "    print(\"üìä Experimento Baseline: Caracter√≠sticas originales\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Crear experimento con caracter√≠sticas originales\n",
    "    experiments = {\n",
    "        'original_features': (X_train, X_test)\n",
    "    }\n",
    "    \n",
    "    # Evaluar experimento baseline\n",
    "    results = evaluate_multiple_experiments(experiments, \"baseline\", y_train, y_test)\n",
    "    \n",
    "    print(f\"\\nBaseline establecido con {X_train.shape[1]} caracter√≠sticas originales\")\n",
    "    print(f\"Accuracy baseline: {results['original_features']['accuracy']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5a373",
   "metadata": {},
   "source": [
    "### Experimento 1: Caracter√≠sticas basadas en √°rboles de decisi√≥n\n",
    "    \n",
    "Este experimento utiliza modelos de √°rboles (XGBoost y Random Forest) para generar nuevas caracter√≠sticas basadas en:\n",
    "- √çndices de hojas donde terminan las muestras\n",
    "- Combinaciones de m√∫ltiples modelos de √°rboles\n",
    "- Estad√≠sticas derivadas de las estructuras de √°rboles\n",
    "    \n",
    "Los modelos de √°rboles capturan patrones no lineales complejos y sus estructuras internas pueden usarse como caracter√≠sticas transformadas.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f510d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tree_based_features(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Par√°metros:\n",
    "    - X_train: DataFrame con caracter√≠sticas de entrenamiento\n",
    "    - X_test: DataFrame con caracter√≠sticas de test\n",
    "    - y_train: Series con etiquetas de entrenamiento\n",
    "    - y_test: Series con etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con resultados de todas las variaciones del experimento\n",
    "    \"\"\"\n",
    "    print(\"üå≥ Experimento 1: Caracter√≠sticas basadas en √°rboles\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1.1 XGBoost leaf indices\n",
    "    # Entrena un modelo XGBoost y extrae los √≠ndices de las hojas donde\n",
    "    # terminan las muestras. Cada √°rbol contribuye con un √≠ndice de hoja.\n",
    "    print(\"Generando caracter√≠sticas con XGBoost leaf indices...\")\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # apply() devuelve los √≠ndices de hojas para cada muestra en cada √°rbol\n",
    "    leaf_indices_train = xgb_model.apply(X_train)\n",
    "    leaf_indices_test = xgb_model.apply(X_test)\n",
    "    \n",
    "    # 1.2 Random Forest leaf indices\n",
    "    # Similar a XGBoost pero usando Random Forest, que puede capturar\n",
    "    # patrones diferentes debido a su estrategia de bagging\n",
    "    print(\"Generando caracter√≠sticas con Random Forest leaf indices...\")\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # apply() devuelve los √≠ndices de hojas para cada √°rbol del bosque\n",
    "    rf_leaf_train = rf_model.apply(X_train)\n",
    "    rf_leaf_test = rf_model.apply(X_test)\n",
    "    \n",
    "    # 1.3 Combinaci√≥n de ambos modelos\n",
    "    # Concatena las caracter√≠sticas de ambos modelos para capturar\n",
    "    # la informaci√≥n complementaria de XGBoost y Random Forest\n",
    "    print(\"Combinando caracter√≠sticas de XGBoost y Random Forest...\")\n",
    "    combined_leaves_train = np.hstack([leaf_indices_train, rf_leaf_train])\n",
    "    combined_leaves_test = np.hstack([leaf_indices_test, rf_leaf_test])\n",
    "    \n",
    "    # 1.4 Estad√≠sticas derivadas: distancias promedio a hojas\n",
    "    # Calcula el promedio de los √≠ndices de hojas como una medida\n",
    "    # de la \"profundidad promedio\" o \"complejidad\" de la decisi√≥n\n",
    "    print(\"Calculando estad√≠sticas derivadas de hojas...\")\n",
    "    leaf_distances_train = np.mean(leaf_indices_train, axis=1).reshape(-1, 1)\n",
    "    leaf_distances_test = np.mean(leaf_indices_test, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Preparar experimentos para evaluaci√≥n\n",
    "    experiments = {\n",
    "        'xgb_leaves': (leaf_indices_train, leaf_indices_test),\n",
    "        'rf_leaves': (rf_leaf_train, rf_leaf_test),\n",
    "        'combined_leaves': (combined_leaves_train, combined_leaves_test),\n",
    "        'leaf_distances': (leaf_distances_train, leaf_distances_test)\n",
    "    }\n",
    "    \n",
    "    # Evaluar todas las variaciones\n",
    "    results = evaluate_multiple_experiments(experiments, \"tree_based\", y_train, y_test)\n",
    "    \n",
    "    print(f\"\\nMejor resultado en tree-based features:\")\n",
    "    best_method = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"  {best_method[0]}: {best_method[1]['accuracy']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c80e0",
   "metadata": {},
   "source": [
    "### Experimento 2: Reducci√≥n de dimensionalidad\n",
    "    \n",
    "Este experimento aplica diferentes t√©cnicas de reducci√≥n de dimensionalidad para crear representaciones m√°s compactas y potencialmente m√°s informativas de los datos:\n",
    "    \n",
    "- PCA: Encuentra componentes principales que maximizan la varianza\n",
    "- TruncatedSVD: Descomposici√≥n en valores singulares, √∫til para datos sparse\n",
    "- FastICA: An√°lisis de componentes independientes, separa se√±ales mezcladas\n",
    "- Combinaci√≥n: Usa caracter√≠sticas originales + PCA para enriquecimiento\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "278f7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_dimensionality_reduction(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Par√°metros:\n",
    "    - X_train: DataFrame con caracter√≠sticas de entrenamiento\n",
    "    - X_test: DataFrame con caracter√≠sticas de test\n",
    "    - y_train: Series con etiquetas de entrenamiento\n",
    "    - y_test: Series con etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con resultados de todas las variaciones del experimento\n",
    "    \"\"\"\n",
    "    print(\"üìâ Experimento 2: Reducci√≥n de dimensionalidad\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 2.1 PCA (Principal Component Analysis)\n",
    "    # Encuentra las direcciones de m√°xima varianza en los datos\n",
    "    # √ötil para eliminar ruido y correlaciones lineales\n",
    "    print(\"Aplicando PCA...\")\n",
    "    n_components_pca = min(50, X_train.shape[1])\n",
    "    pca = PCA(n_components=n_components_pca)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    print(f\"  PCA: {X_train.shape[1]} ‚Üí {n_components_pca} componentes\")\n",
    "    print(f\"  Varianza explicada: {pca.explained_variance_ratio_[:5].sum():.3f} (primeros 5 componentes)\")\n",
    "    \n",
    "    # 2.2 TruncatedSVD (Singular Value Decomposition)\n",
    "    # Similar a PCA pero no centra los datos, mejor para matrices sparse\n",
    "    # √ötil cuando los datos tienen muchos ceros o est√°n normalizados\n",
    "    print(\"Aplicando TruncatedSVD...\")\n",
    "    n_components_svd = min(50, X_train.shape[1]-1)\n",
    "    svd = TruncatedSVD(n_components=n_components_svd, random_state=42)\n",
    "    X_train_svd = svd.fit_transform(X_train)\n",
    "    X_test_svd = svd.transform(X_test)\n",
    "    \n",
    "    print(f\"  SVD: {X_train.shape[1]} ‚Üí {n_components_svd} componentes\")\n",
    "    print(f\"  Varianza explicada: {svd.explained_variance_ratio_[:5].sum():.3f} (primeros 5 componentes)\")\n",
    "    \n",
    "    # 2.3 FastICA (Independent Component Analysis)\n",
    "    # Separa se√±ales mezcladas asumiendo independencia estad√≠stica\n",
    "    # √ötil para encontrar fuentes de variaci√≥n independientes\n",
    "    print(\"Aplicando FastICA...\")\n",
    "    n_components_ica = min(20, X_train.shape[1])\n",
    "    ica = FastICA(n_components=n_components_ica, random_state=42)\n",
    "    X_train_ica = ica.fit_transform(X_train)\n",
    "    X_test_ica = ica.transform(X_test)\n",
    "    \n",
    "    print(f\"  ICA: {X_train.shape[1]} ‚Üí {n_components_ica} componentes independientes\")\n",
    "    \n",
    "    # 2.4 Combinaci√≥n: Caracter√≠sticas originales + PCA\n",
    "    # Mantiene la informaci√≥n original y a√±ade componentes principales\n",
    "    # Estrategia de enriquecimiento que puede capturar tanto patrones\n",
    "    # lineales (PCA) como no lineales (caracter√≠sticas originales)\n",
    "    print(\"Combinando caracter√≠sticas originales con PCA...\")\n",
    "    X_train_combined = np.hstack([X_train, X_train_pca])\n",
    "    X_test_combined = np.hstack([X_test, X_test_pca])\n",
    "    \n",
    "    print(f\"  Combinado: {X_train.shape[1]} + {n_components_pca} = {X_train_combined.shape[1]} caracter√≠sticas\")\n",
    "    \n",
    "    # Preparar experimentos para evaluaci√≥n\n",
    "    experiments = {\n",
    "        'pca_only': (X_train_pca, X_test_pca),\n",
    "        'svd_only': (X_train_svd, X_test_svd),\n",
    "        'ica_only': (X_train_ica, X_test_ica),\n",
    "        'original_plus_pca': (X_train_combined, X_test_combined)\n",
    "    }\n",
    "    \n",
    "    # Evaluar todas las variaciones\n",
    "    results = evaluate_multiple_experiments(experiments, \"dimensionality_reduction\", y_train, y_test)\n",
    "    \n",
    "    print(f\"\\nMejor resultado en reducci√≥n de dimensionalidad:\")\n",
    "    best_method = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"  {best_method[0]}: {best_method[1]['accuracy']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd6e84",
   "metadata": {},
   "source": [
    "### Experimento 3: Interacciones entre caracter√≠sticas\n",
    "    \n",
    "Este experimento crea nuevas caracter√≠sticas basadas en las interacciones entre las caracter√≠sticas existentes:\n",
    "    \n",
    "- Caracter√≠sticas polin√≥micas: Productos entre pares de caracter√≠sticas\n",
    "- Interacciones manuales: Productos selectivos entre caracter√≠sticas principales\n",
    "- Ratios: Divisiones entre caracter√≠sticas para capturar proporciones\n",
    "    \n",
    "Las interacciones pueden revelar patrones que no son evidentes cuando se consideran las caracter√≠sticas individualmente.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17c3fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_feature_interactions(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Par√°metros:\n",
    "    - X_train: DataFrame con caracter√≠sticas de entrenamiento\n",
    "    - X_test: DataFrame con caracter√≠sticas de test\n",
    "    - y_train: Series con etiquetas de entrenamiento\n",
    "    - y_test: Series con etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con resultados de todas las variaciones del experimento\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Experimento 3: Interacciones entre caracter√≠sticas\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 3.1 Caracter√≠sticas polin√≥micas (grado 2)\n",
    "    # Genera autom√°ticamente todos los productos entre pares de caracter√≠sticas\n",
    "    # include_bias=False: no incluye t√©rmino constante\n",
    "    # interaction_only=True: solo productos cruzados, no cuadrados\n",
    "    print(\"Generando caracter√≠sticas polin√≥micas...\")\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    print(f\"  Polin√≥micas: {X_train.shape[1]} ‚Üí {X_train_poly.shape[1]} caracter√≠sticas\")\n",
    "    \n",
    "    # 3.2 Interacciones manuales entre caracter√≠sticas principales\n",
    "    # Selecciona las primeras N caracter√≠sticas y genera productos entre ellas\n",
    "    # M√°s controlado que PolynomialFeatures, evita explosi√≥n de caracter√≠sticas\n",
    "    print(\"Generando interacciones manuales...\")\n",
    "    n_features = min(10, X_train.shape[1])\n",
    "    interactions_train = []\n",
    "    interactions_test = []\n",
    "    \n",
    "    interaction_count = 0\n",
    "    for i in range(n_features):\n",
    "        for j in range(i+1, n_features):\n",
    "            # Producto entre caracter√≠sticas i y j\n",
    "            interaction_train = (X_train.iloc[:, i] * X_train.iloc[:, j]).values\n",
    "            interaction_test = (X_test.iloc[:, i] * X_test.iloc[:, j]).values\n",
    "            \n",
    "            interactions_train.append(interaction_train)\n",
    "            interactions_test.append(interaction_test)\n",
    "            interaction_count += 1\n",
    "    \n",
    "    X_train_interactions = np.column_stack(interactions_train)\n",
    "    X_test_interactions = np.column_stack(interactions_test)\n",
    "    \n",
    "    print(f\"  Interacciones manuales: {interaction_count} nuevas caracter√≠sticas\")\n",
    "    \n",
    "    # 3.3 Ratios entre caracter√≠sticas\n",
    "    # Calcula ratios (divisiones) entre caracter√≠sticas para capturar proporciones\n",
    "    # √ötil para detectar patrones relativos entre variables\n",
    "    print(\"Generando ratios entre caracter√≠sticas...\")\n",
    "    ratios_train = []\n",
    "    ratios_test = []\n",
    "    \n",
    "    ratio_count = 0\n",
    "    n_features_ratio = min(5, X_train.shape[1])\n",
    "    \n",
    "    for i in range(n_features_ratio):\n",
    "        for j in range(i+1, n_features_ratio):\n",
    "            # Evitar divisi√≥n por cero agregando peque√±a constante\n",
    "            denominator_train = X_train.iloc[:, j] + 1e-8\n",
    "            denominator_test = X_test.iloc[:, j] + 1e-8\n",
    "            \n",
    "            # Ratio entre caracter√≠sticas i y j\n",
    "            ratio_train = (X_train.iloc[:, i] / denominator_train).values\n",
    "            ratio_test = (X_test.iloc[:, i] / denominator_test).values\n",
    "            \n",
    "            ratios_train.append(ratio_train)\n",
    "            ratios_test.append(ratio_test)\n",
    "            ratio_count += 1\n",
    "    \n",
    "    X_train_ratios = np.column_stack(ratios_train)\n",
    "    X_test_ratios = np.column_stack(ratios_test)\n",
    "    \n",
    "    print(f\"  Ratios: {ratio_count} nuevas caracter√≠sticas\")\n",
    "    \n",
    "    # Preparar experimentos para evaluaci√≥n\n",
    "    experiments = {\n",
    "        'polynomial_features': (X_train_poly, X_test_poly),\n",
    "        'manual_interactions': (X_train_interactions, X_test_interactions),\n",
    "        'feature_ratios': (X_train_ratios, X_test_ratios)\n",
    "    }\n",
    "    \n",
    "    # Evaluar todas las variaciones\n",
    "    results = evaluate_multiple_experiments(experiments, \"feature_interactions\", y_train, y_test)\n",
    "    \n",
    "    print(f\"\\nMejor resultado en interacciones de caracter√≠sticas:\")\n",
    "    best_method = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"  {best_method[0]}: {best_method[1]['accuracy']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa116c",
   "metadata": {},
   "source": [
    "### Experimento 4: Caracter√≠sticas basadas en clustering\n",
    "    \n",
    "Este experimento utiliza t√©cnicas de clustering para crear nuevas caracter√≠sticas basadas en la estructura de agrupamiento de los datos:\n",
    "    \n",
    "- Etiquetas de cluster: Asignaci√≥n de muestras a grupos\n",
    "- Distancias a centroides: Qu√© tan lejos est√° cada muestra de los centros\n",
    "- Clustering por clase: Distancias a centroides espec√≠ficos de cada clase\n",
    "    \n",
    "El clustering puede revelar patrones de agrupamiento natural en los datos que pueden ser √∫tiles para la clasificaci√≥n.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cae4b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_clustering_features(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Par√°metros:\n",
    "    - X_train: DataFrame con caracter√≠sticas de entrenamiento\n",
    "    - X_test: DataFrame con caracter√≠sticas de test\n",
    "    - y_train: Series con etiquetas de entrenamiento\n",
    "    - y_test: Series con etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con resultados de todas las variaciones del experimento\n",
    "    \"\"\"\n",
    "    print(\"üéØ Experimento 4: Caracter√≠sticas basadas en clustering\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 4.1 KMeans clustering b√°sico\n",
    "    # Agrupa los datos en k clusters y usa las etiquetas como caracter√≠sticas\n",
    "    print(\"Aplicando K-Means clustering...\")\n",
    "    n_clusters = 10\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels_train = kmeans.fit_predict(X_train)\n",
    "    cluster_labels_test = kmeans.predict(X_test)\n",
    "    \n",
    "    print(f\"  K-Means: {n_clusters} clusters creados\")\n",
    "    print(f\"  Distribuci√≥n de clusters en entrenamiento: {np.bincount(cluster_labels_train)}\")\n",
    "    \n",
    "    # 4.2 Distancias a centroides de K-Means\n",
    "    # Calcula la distancia de cada muestra a cada centroide\n",
    "    # Proporciona informaci√≥n m√°s rica que solo las etiquetas\n",
    "    print(\"Calculando distancias a centroides...\")\n",
    "    distances_train = kmeans.transform(X_train)\n",
    "    distances_test = kmeans.transform(X_test)\n",
    "    \n",
    "    print(f\"  Distancias: {distances_train.shape[1]} caracter√≠sticas de distancia\")\n",
    "    \n",
    "    # 4.3 Clustering espec√≠fico por clase\n",
    "    # Calcula centroides separados para cada clase y mide distancias\n",
    "    # √ötil para detectar qu√© tan similar es una muestra a cada clase\n",
    "    print(\"Calculando centroides por clase...\")\n",
    "    \n",
    "    # Separar datos por clase\n",
    "    fraud_mask = (y_train == 1)\n",
    "    normal_mask = (y_train == 0)\n",
    "    \n",
    "    fraud_data = X_train[fraud_mask]\n",
    "    normal_data = X_train[normal_mask]\n",
    "    \n",
    "    print(f\"  Datos de fraude: {fraud_data.shape[0]} muestras\")\n",
    "    print(f\"  Datos normales: {normal_data.shape[0]} muestras\")\n",
    "    \n",
    "    # Calcular centroides por clase\n",
    "    fraud_centroid = fraud_data.mean().values\n",
    "    normal_centroid = normal_data.mean().values\n",
    "    \n",
    "    # Distancias a centroide de clase fraudulenta\n",
    "    fraud_dist_train = np.linalg.norm(X_train - fraud_centroid, axis=1).reshape(-1, 1)\n",
    "    fraud_dist_test = np.linalg.norm(X_test - fraud_centroid, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Distancias a centroide de clase normal\n",
    "    normal_dist_train = np.linalg.norm(X_train - normal_centroid, axis=1).reshape(-1, 1)\n",
    "    normal_dist_test = np.linalg.norm(X_test - normal_centroid, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Combinar distancias a ambas clases\n",
    "    class_distances_train = np.hstack([fraud_dist_train, normal_dist_train])\n",
    "    class_distances_test = np.hstack([fraud_dist_test, normal_dist_test])\n",
    "    \n",
    "    print(f\"  Distancias por clase: 2 caracter√≠sticas (dist_fraud, dist_normal)\")\n",
    "    \n",
    "    # Estad√≠sticas adicionales de distancias\n",
    "    print(f\"  Distancia promedio a centroide fraude: {np.mean(fraud_dist_train):.3f}\")\n",
    "    print(f\"  Distancia promedio a centroide normal: {np.mean(normal_dist_train):.3f}\")\n",
    "    \n",
    "    # Preparar experimentos para evaluaci√≥n\n",
    "    experiments = {\n",
    "        'kmeans_labels': (cluster_labels_train.reshape(-1, 1), cluster_labels_test.reshape(-1, 1)),\n",
    "        'kmeans_distances': (distances_train, distances_test),\n",
    "        'class_distances': (class_distances_train, class_distances_test)\n",
    "    }\n",
    "    \n",
    "    # Evaluar todas las variaciones\n",
    "    results = evaluate_multiple_experiments(experiments, \"clustering\", y_train, y_test)\n",
    "    \n",
    "    print(f\"\\nMejor resultado en caracter√≠sticas de clustering:\")\n",
    "    best_method = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"  {best_method[0]}: {best_method[1]['accuracy']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2355395",
   "metadata": {},
   "source": [
    "### Experimento 5: Caracter√≠sticas de detecci√≥n de anomal√≠as\n",
    "    \n",
    "Este experimento utiliza t√©cnicas de detecci√≥n de anomal√≠as para crear caracter√≠sticas que midan qu√© tan \"an√≥mala\" o \"inusual\" es cada muestra:\n",
    "    \n",
    "- Isolation Forest: A√≠sla anomal√≠as usando √°rboles aleatorios\n",
    "- Distancia de Mahalanobis: Distancia considerando covarianza\n",
    "- Estad√≠sticas locales: Z-scores y medidas de desviaci√≥n\n",
    "    \n",
    "Estas caracter√≠sticas son especialmente √∫tiles para detecci√≥n de fraude, ya que los fraudes suelen ser anomal√≠as en los patrones normales.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18983490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_anomaly_features(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Par√°metros:\n",
    "    - X_train: DataFrame con caracter√≠sticas de entrenamiento\n",
    "    - X_test: DataFrame con caracter√≠sticas de test\n",
    "    - y_train: Series con etiquetas de entrenamiento\n",
    "    - y_test: Series con etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con resultados de todas las variaciones del experimento\n",
    "    \"\"\"\n",
    "    print(\"üö® Experimento 5: Caracter√≠sticas de detecci√≥n de anomal√≠as\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 5.1 Isolation Forest scores\n",
    "    # Isolation Forest a√≠sla anomal√≠as usando √°rboles de decisi√≥n aleatorios\n",
    "    # Los puntos an√≥malos requieren menos divisiones para ser aislados\n",
    "    print(\"Aplicando Isolation Forest...\")\n",
    "    iso_forest = IsolationForest(n_estimators=100, random_state=42)\n",
    "    iso_forest.fit(X_train)\n",
    "    \n",
    "    # decision_function devuelve scores de anomal√≠a (m√°s negativo = m√°s an√≥malo)\n",
    "    anomaly_scores_train = iso_forest.decision_function(X_train).reshape(-1, 1)\n",
    "    anomaly_scores_test = iso_forest.decision_function(X_test).reshape(-1, 1)\n",
    "    \n",
    "    print(f\"  Isolation Forest: 1 caracter√≠stica de anomal√≠a\")\n",
    "    print(f\"  Score promedio entrenamiento: {np.mean(anomaly_scores_train):.3f}\")\n",
    "    print(f\"  Score promedio test: {np.mean(anomaly_scores_test):.3f}\")\n",
    "    \n",
    "    # 5.2 Distancia de Mahalanobis\n",
    "    # Mide distancia considerando la covarianza entre caracter√≠sticas\n",
    "    # M√°s robusta que distancia euclidiana para datos correlacionados\n",
    "    print(\"Calculando distancias de Mahalanobis...\")\n",
    "    \n",
    "    # Calcular matriz de covarianza y su inversa\n",
    "    cov_matrix = np.cov(X_train.T)\n",
    "    \n",
    "    # Usar pseudoinversa para manejar matrices singulares\n",
    "    inv_cov = np.linalg.pinv(cov_matrix)\n",
    "    mean_vector = np.mean(X_train, axis=0)\n",
    "    \n",
    "    def mahalanobis_distance(X, mean, inv_cov):\n",
    "        \"\"\"\n",
    "        Calcula la distancia de Mahalanobis para cada muestra\n",
    "        \"\"\"\n",
    "        diff = X - mean\n",
    "        return np.sqrt(np.sum(diff @ inv_cov * diff, axis=1))\n",
    "    \n",
    "    # Calcular distancias de Mahalanobis\n",
    "    mahal_dist_train = np.array(mahalanobis_distance(X_train, mean_vector, inv_cov)).reshape(-1, 1)\n",
    "    mahal_dist_test = np.array(mahalanobis_distance(X_test, mean_vector, inv_cov)).reshape(-1, 1)\n",
    "\n",
    "    print(f\"  Mahalanobis: 1 caracter√≠stica de distancia\")\n",
    "    print(f\"  Distancia promedio entrenamiento: {np.mean(mahal_dist_train):.3f}\")\n",
    "    print(f\"  Distancia promedio test: {np.mean(mahal_dist_test):.3f}\")\n",
    "    \n",
    "    # 5.3 Estad√≠sticas locales por caracter√≠stica\n",
    "    # Calcula Z-scores para cada caracter√≠stica individual\n",
    "    # Identifica muestras que son outliers en caracter√≠sticas espec√≠ficas\n",
    "    print(\"Calculando estad√≠sticas locales (Z-scores)...\")\n",
    "    local_stats_train = []\n",
    "    local_stats_test = []\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        # Calcular media y desviaci√≥n est√°ndar de entrenamiento\n",
    "        col_mean = X_train[col].mean()\n",
    "        col_std = X_train[col].std()\n",
    "        \n",
    "        # Z-score: (valor - media) / desviaci√≥n est√°ndar\n",
    "        z_score_train = (X_train[col] - col_mean) / col_std\n",
    "        z_score_test = (X_test[col] - col_mean) / col_std\n",
    "        \n",
    "        local_stats_train.append(z_score_train.values)\n",
    "        local_stats_test.append(z_score_test.values)\n",
    "    \n",
    "    local_stats_train = np.column_stack(local_stats_train)\n",
    "    local_stats_test = np.column_stack(local_stats_test)\n",
    "    \n",
    "    print(f\"  Estad√≠sticas locales: {local_stats_train.shape[1]} caracter√≠sticas Z-score\")\n",
    "    \n",
    "    # 5.4 Estad√≠sticas agregadas de anomal√≠a\n",
    "    # Combina m√∫ltiples medidas de anomal√≠a en caracter√≠sticas √∫nicas\n",
    "    print(\"Creando estad√≠sticas agregadas...\")\n",
    "    \n",
    "    # M√°ximo Z-score absoluto por muestra (outlier m√°s extremo)\n",
    "    max_zscore_train = np.max(np.abs(local_stats_train), axis=1).reshape(-1, 1)\n",
    "    max_zscore_test = np.max(np.abs(local_stats_test), axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # N√∫mero de caracter√≠sticas con |Z-score| > 2 (outliers moderados)\n",
    "    outlier_count_train = np.sum(np.abs(local_stats_train) > 2, axis=1).reshape(-1, 1)\n",
    "    outlier_count_test = np.sum(np.abs(local_stats_test) > 2, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Combinar estad√≠sticas agregadas\n",
    "    aggregated_stats_train = np.hstack([max_zscore_train, outlier_count_train])\n",
    "    aggregated_stats_test = np.hstack([max_zscore_test, outlier_count_test])\n",
    "    \n",
    "    print(f\"  Estad√≠sticas agregadas: {aggregated_stats_train.shape[1]} caracter√≠sticas\")\n",
    "    \n",
    "    # Preparar experimentos para evaluaci√≥n\n",
    "    experiments = {\n",
    "        'isolation_forest': (anomaly_scores_train, anomaly_scores_test),\n",
    "        'mahalanobis_distance': (mahal_dist_train, mahal_dist_test),\n",
    "        'local_statistics': (local_stats_train, local_stats_test),\n",
    "        'aggregated_stats': (aggregated_stats_train, aggregated_stats_test)\n",
    "    }\n",
    "    \n",
    "    # Evaluar todas las variaciones\n",
    "    results = evaluate_multiple_experiments(experiments, \"anomaly_detection\", y_train, y_test)\n",
    "    \n",
    "    print(f\"\\nMejor resultado en detecci√≥n de anomal√≠as:\")\n",
    "    best_method = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"  {best_method[0]}: {best_method[1]['accuracy']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8212abd",
   "metadata": {},
   "source": [
    "### Experimento 6: Selecci√≥n de caracter√≠sticas\n",
    "    \n",
    "Este experimento utiliza diferentes t√©cnicas de selecci√≥n de caracter√≠sticas para identificar y mantener solo las m√°s relevantes:\n",
    "    \n",
    "- Mutual Information: Mide dependencia estad√≠stica entre caracter√≠sticas y target\n",
    "- F-score: Estad√≠stico F de ANOVA para clasificaci√≥n\n",
    "- RFE: Eliminaci√≥n recursiva usando Random Forest\n",
    "- SelectFromModel: Selecci√≥n basada en importancia de XGBoost\n",
    "    \n",
    "La selecci√≥n de caracter√≠sticas puede mejorar el rendimiento eliminando ruido y caracter√≠sticas irrelevantes, y reduce el riesgo de overfitting.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9093cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_feature_selection(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Par√°metros:\n",
    "    - X_train: DataFrame con caracter√≠sticas de entrenamiento\n",
    "    - X_test: DataFrame con caracter√≠sticas de test\n",
    "    - y_train: Series con etiquetas de entrenamiento\n",
    "    - y_test: Series con etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con resultados de todas las variaciones del experimento\n",
    "    \"\"\"\n",
    "    print(\"üéØ Experimento 6: Selecci√≥n de caracter√≠sticas\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 6.1 Mutual Information\n",
    "    # Mide la dependencia estad√≠stica entre cada caracter√≠stica y el target\n",
    "    # Captura relaciones tanto lineales como no lineales\n",
    "    print(\"Aplicando selecci√≥n por Mutual Information...\")\n",
    "    k_features_mi = min(20, X_train.shape[1])\n",
    "    \n",
    "    selector_mi = SelectKBest(score_func=mutual_info_classif, k=k_features_mi)\n",
    "    X_train_mi = selector_mi.fit_transform(X_train, y_train)\n",
    "    X_test_mi = selector_mi.transform(X_test)\n",
    "    \n",
    "    # Obtener scores de mutual information\n",
    "    mi_scores = selector_mi.scores_\n",
    "    selected_features_mi = X_train.columns[selector_mi.get_support()]\n",
    "    \n",
    "    print(f\"  Mutual Information: {X_train.shape[1]} ‚Üí {k_features_mi} caracter√≠sticas\")\n",
    "    print(f\"  Score promedio seleccionadas: {np.mean(mi_scores[selector_mi.get_support()]):.3f}\")\n",
    "    print(f\"  Top 3 caracter√≠sticas: {selected_features_mi[:3].tolist()}\")\n",
    "    \n",
    "    # 6.2 F-score (ANOVA F-test)\n",
    "    # Estad√≠stico F que mide la diferencia entre medias de grupos\n",
    "    # Efectivo para caracter√≠sticas con relaciones lineales con el target\n",
    "    print(\"Aplicando selecci√≥n por F-score...\")\n",
    "    k_features_f = min(20, X_train.shape[1])\n",
    "    \n",
    "    selector_f = SelectKBest(score_func=f_classif, k=k_features_f)\n",
    "    X_train_f = selector_f.fit_transform(X_train, y_train)\n",
    "    X_test_f = selector_f.transform(X_test)\n",
    "    \n",
    "    # Obtener F-scores\n",
    "    f_scores = selector_f.scores_\n",
    "    selected_features_f = X_train.columns[selector_f.get_support()]\n",
    "    \n",
    "    print(f\"  F-score: {X_train.shape[1]} ‚Üí {k_features_f} caracter√≠sticas\")\n",
    "    print(f\"  F-score promedio seleccionadas: {np.mean(f_scores[selector_f.get_support()]):.3f}\")\n",
    "    print(f\"  Top 3 caracter√≠sticas: {selected_features_f[:3].tolist()}\")\n",
    "    \n",
    "    # 6.3 RFE (Recursive Feature Elimination) con Random Forest\n",
    "    # Elimina caracter√≠sticas recursivamente bas√°ndose en importancia del modelo\n",
    "    # Considera interacciones entre caracter√≠sticas\n",
    "    print(\"Aplicando RFE con Random Forest...\")\n",
    "    n_features_rfe = min(15, X_train.shape[1])\n",
    "    \n",
    "    # Usar Random Forest como estimador base\n",
    "    rf_estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    rf_selector = RFE(rf_estimator, n_features_to_select=n_features_rfe)\n",
    "    \n",
    "    X_train_rfe = rf_selector.fit_transform(X_train, y_train)\n",
    "    X_test_rfe = rf_selector.transform(X_test)\n",
    "    \n",
    "    # Obtener caracter√≠sticas seleccionadas\n",
    "    selected_features_rfe = X_train.columns[rf_selector.get_support()]\n",
    "    feature_rankings = rf_selector.ranking_\n",
    "    \n",
    "    print(f\"  RFE: {X_train.shape[1]} ‚Üí {n_features_rfe} caracter√≠sticas\")\n",
    "    print(f\"  Caracter√≠sticas seleccionadas: {selected_features_rfe[:3].tolist()}\")\n",
    "    \n",
    "    # 6.4 SelectFromModel con XGBoost\n",
    "    # Selecciona caracter√≠sticas bas√°ndose en importancia de XGBoost\n",
    "    # Autom√°ticamente determina el umbral √≥ptimo\n",
    "    print(\"Aplicando SelectFromModel con XGBoost...\")\n",
    "    \n",
    "    # Entrenar XGBoost para obtener importancias\n",
    "    xgb_estimator = xgb.XGBClassifier(n_estimators=50, random_state=42)\n",
    "    xgb_selector = SelectFromModel(xgb_estimator)\n",
    "    \n",
    "    X_train_xgb = xgb_selector.fit_transform(X_train, y_train)\n",
    "    X_test_xgb = xgb_selector.transform(X_test)\n",
    "    \n",
    "    # Obtener informaci√≥n sobre la selecci√≥n\n",
    "    selected_features_xgb = X_train.columns[xgb_selector.get_support()]\n",
    "    feature_importances = xgb_selector.estimator_.feature_importances_\n",
    "    \n",
    "    print(f\"  XGBoost: {X_train.shape[1]} ‚Üí {X_train_xgb.shape[1]} caracter√≠sticas\")\n",
    "    print(f\"  Umbral autom√°tico: {xgb_selector.threshold_:.4f}\")\n",
    "    print(f\"  Top 3 caracter√≠sticas: {selected_features_xgb[:3].tolist()}\")\n",
    "    \n",
    "    # 6.5 An√°lisis de consenso\n",
    "    # Identifica caracter√≠sticas seleccionadas por m√∫ltiples m√©todos\n",
    "    print(\"Analizando consenso entre m√©todos...\")\n",
    "    \n",
    "    all_selected = set(selected_features_mi) | set(selected_features_f) | \\\n",
    "                  set(selected_features_rfe) | set(selected_features_xgb)\n",
    "    \n",
    "    consensus_features = set(selected_features_mi) & set(selected_features_f) & \\\n",
    "                        set(selected_features_rfe) & set(selected_features_xgb)\n",
    "    \n",
    "    print(f\"  Caracter√≠sticas √∫nicas total: {len(all_selected)}\")\n",
    "    print(f\"  Caracter√≠sticas en consenso: {len(consensus_features)}\")\n",
    "    if len(consensus_features) > 0:\n",
    "        print(f\"  Consenso: {list(consensus_features)[:5]}\")\n",
    "    \n",
    "    # Preparar experimentos para evaluaci√≥n\n",
    "    experiments = {\n",
    "        'mutual_info_selection': (X_train_mi, X_test_mi),\n",
    "        'f_score_selection': (X_train_f, X_test_f),\n",
    "        'rfe_selection': (X_train_rfe, X_test_rfe),\n",
    "        'xgb_selection': (X_train_xgb, X_test_xgb)\n",
    "    }\n",
    "    \n",
    "    # Evaluar todas las variaciones\n",
    "    results = evaluate_multiple_experiments(experiments, \"feature_selection\", y_train, y_test)\n",
    "    \n",
    "    print(f\"\\nMejor resultado en selecci√≥n de caracter√≠sticas:\")\n",
    "    best_method = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"  {best_method[0]}: {best_method[1]['accuracy']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3165386",
   "metadata": {},
   "source": [
    "### Funciones de evaluaci√≥n, resumen y ejecuci√≥n de experimentos\n",
    "\n",
    "En este bloque se agrupan las funciones principales para la gesti√≥n de los experimentos de ingenier√≠a de caracter√≠sticas:\n",
    "\n",
    "- **evaluate_experiment:** Eval√∫a un conjunto de caracter√≠sticas transformadas usando un modelo de regresi√≥n log√≠stica y calcula el accuracy.\n",
    "- **evaluate_multiple_experiments:** Permite evaluar varias variantes de un mismo tipo de experimento y organiza los resultados en el diccionario global.\n",
    "- **print_experiment_summary:** Muestra un resumen de los resultados de todos los experimentos de ingenier√≠a de caracter√≠sticas, incluyendo ranking, estad√≠sticas y recomendaciones.\n",
    "- **run_all_experiments:** Ejecuta en secuencia todos los experimentos de feature engineering, almacena los resultados globales y los guarda en un archivo JSON para su posterior an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e717428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experiment(X_train_exp, X_test_exp, y_train, y_test, experiment_name):\n",
    "    \"\"\"\n",
    "    Funci√≥n auxiliar para evaluar un experimento espec√≠fico usando Regresi√≥n Log√≠stica\n",
    "    \n",
    "    Par√°metros:\n",
    "    - X_train_exp: Datos de entrenamiento transformados\n",
    "    - X_test_exp: Datos de test transformados  \n",
    "    - y_train: Etiquetas de entrenamiento\n",
    "    - y_test: Etiquetas de test\n",
    "    - experiment_name: Nombre del experimento para mostrar en resultados\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con accuracy, n√∫mero de features y objetos entrenados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Escalado de caracter√≠sticas usando StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_exp)\n",
    "        X_test_scaled = scaler.transform(X_test_exp)\n",
    "        \n",
    "        # Entrenamiento del modelo de Regresi√≥n Log√≠stica\n",
    "        lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        lr.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predicci√≥n y evaluaci√≥n\n",
    "        y_pred = lr.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        result = {\n",
    "            'accuracy': accuracy,\n",
    "            'n_features': X_train_exp.shape[1],\n",
    "            'model': lr,\n",
    "            'scaler': scaler\n",
    "        }\n",
    "        \n",
    "        print(f\"  {experiment_name}: {accuracy:.4f} (features: {X_train_exp.shape[1]})\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  {experiment_name}: Error - {str(e)}\")\n",
    "        return {'accuracy': 0, 'error': str(e)}\n",
    "\n",
    "def evaluate_multiple_experiments(experiments, experiment_type, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Funci√≥n auxiliar para evaluar m√∫ltiples variaciones de un tipo de experimento\n",
    "    \n",
    "    Par√°metros:\n",
    "    - experiments: Diccionario con nombre_experimento: (X_train, X_test)\n",
    "    - experiment_type: Tipo de experimento (para organizar resultados)\n",
    "    - y_train: Etiquetas de entrenamiento\n",
    "    - y_test: Etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con resultados de todas las variaciones\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, (X_train_exp, X_test_exp) in experiments.items():\n",
    "        result = evaluate_experiment(X_train_exp, X_test_exp, y_train, y_test, name)\n",
    "        results[name] = result\n",
    "    \n",
    "    global_results[experiment_type] = results\n",
    "    return results\n",
    "\n",
    "def print_experiment_summary():\n",
    "    \"\"\"\n",
    "    Imprime un resumen completo de todos los experimentos ejecutados\n",
    "    \n",
    "    Analiza los resultados almacenados en global_results y muestra:\n",
    "    - Ranking de todos los m√©todos por accuracy\n",
    "    - Comparaci√≥n de n√∫mero de caracter√≠sticas\n",
    "    - Mejores m√©todos por categor√≠a\n",
    "    - Estad√≠sticas generales de los experimentos\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìã RESUMEN COMPLETO DE EXPERIMENTOS DE FEATURE ENGINEERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not global_results:\n",
    "        print(\"No se han ejecutado experimentos a√∫n.\")\n",
    "        return\n",
    "    \n",
    "    # Recopilar todos los resultados\n",
    "    all_results = []\n",
    "    for exp_type, results in global_results.items():\n",
    "        for method_name, result in results.items():\n",
    "            if 'accuracy' in result and result['accuracy'] > 0:\n",
    "                all_results.append({\n",
    "                    'experiment': exp_type,\n",
    "                    'method': method_name,\n",
    "                    'accuracy': result['accuracy'],\n",
    "                    'n_features': result.get('n_features', 0)\n",
    "                })\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"No hay resultados v√°lidos para mostrar.\")\n",
    "        return\n",
    "    \n",
    "    # Ordenar por accuracy descendente\n",
    "    all_results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "    \n",
    "    # Mostrar ranking general\n",
    "    print(f\"\\nüèÜ RANKING GENERAL (Top 15)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Rank':<5} {'Experiment':<20} {'Method':<25} {'Accuracy':<10} {'Features':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(all_results[:15], 1):\n",
    "        print(f\"{i:<5} {result['experiment']:<20} {result['method']:<25} \"\n",
    "              f\"{result['accuracy']:.4f}    {result['n_features']:<8}\")\n",
    "    \n",
    "    # Estad√≠sticas por categor√≠a\n",
    "    print(f\"\\nüìä MEJORES M√âTODOS POR CATEGOR√çA\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    categories = {}\n",
    "    for result in all_results:\n",
    "        exp_type = result['experiment']\n",
    "        if exp_type not in categories:\n",
    "            categories[exp_type] = []\n",
    "        categories[exp_type].append(result)\n",
    "    \n",
    "    for category, results in categories.items():\n",
    "        best_result = max(results, key=lambda x: x['accuracy'])\n",
    "        print(f\"{category:<20}: {best_result['method']:<25} \"\n",
    "              f\"({best_result['accuracy']:.4f}, {best_result['n_features']} features)\")\n",
    "    \n",
    "    # Estad√≠sticas generales\n",
    "    print(f\"\\nüìà ESTAD√çSTICAS GENERALES\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    accuracies = [r['accuracy'] for r in all_results]\n",
    "    feature_counts = [r['n_features'] for r in all_results]\n",
    "    \n",
    "    print(f\"Total de experimentos ejecutados: {len(all_results)}\")\n",
    "    print(f\"Accuracy promedio: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"Accuracy m√°xima: {np.max(accuracies):.4f}\")\n",
    "    print(f\"Accuracy m√≠nima: {np.min(accuracies):.4f}\")\n",
    "    print(f\"Desviaci√≥n est√°ndar: {np.std(accuracies):.4f}\")\n",
    "    print(f\"N√∫mero promedio de caracter√≠sticas: {np.mean(feature_counts):.1f}\")\n",
    "    print(f\"Rango de caracter√≠sticas: {np.min(feature_counts)} - {np.max(feature_counts)}\")\n",
    "    \n",
    "    # An√°lisis de eficiencia (accuracy vs n√∫mero de caracter√≠sticas)\n",
    "    print(f\"\\n‚ö° AN√ÅLISIS DE EFICIENCIA\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Encontrar m√©todos con alta accuracy y pocas caracter√≠sticas\n",
    "    efficient_methods = [r for r in all_results if r['accuracy'] > np.mean(accuracies) \n",
    "                        and r['n_features'] <= np.mean(feature_counts)]\n",
    "    \n",
    "    if efficient_methods:\n",
    "        print(\"M√©todos eficientes (alta accuracy, pocas caracter√≠sticas):\")\n",
    "        for method in efficient_methods[:5]:\n",
    "            efficiency_score = method['accuracy'] / (method['n_features'] + 1)\n",
    "            print(f\"  {method['method']:<25}: {method['accuracy']:.4f} \"\n",
    "                  f\"({method['n_features']} features, score: {efficiency_score:.4f})\")\n",
    "    \n",
    "    # Recomendaciones\n",
    "    print(f\"\\nüí° RECOMENDACIONES\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    best_overall = all_results[0]\n",
    "    print(f\"ü•á Mejor m√©todo general: {best_overall['method']}\")\n",
    "    print(f\"   Categor√≠a: {best_overall['experiment']}\")\n",
    "    print(f\"   Accuracy: {best_overall['accuracy']:.4f}\")\n",
    "    print(f\"   Caracter√≠sticas: {best_overall['n_features']}\")\n",
    "    \n",
    "    if efficient_methods:\n",
    "        best_efficient = max(efficient_methods, key=lambda x: x['accuracy'])\n",
    "        print(f\"üöÄ Mejor m√©todo eficiente: {best_efficient['method']}\")\n",
    "        print(f\"   Accuracy: {best_efficient['accuracy']:.4f}\")\n",
    "        print(f\"   Caracter√≠sticas: {best_efficient['n_features']}\")\n",
    "\n",
    "def run_all_experiments(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Ejecuta todos los experimentos de Feature Engineering en secuencia\n",
    "    \n",
    "    Esta funci√≥n ejecuta sistem√°ticamente todos los experimentos disponibles:\n",
    "    1. Baseline con caracter√≠sticas originales\n",
    "    2. Caracter√≠sticas basadas en √°rboles\n",
    "    3. Reducci√≥n de dimensionalidad\n",
    "    4. Interacciones entre caracter√≠sticas\n",
    "    5. Caracter√≠sticas de clustering\n",
    "    6. Caracter√≠sticas de detecci√≥n de anomal√≠as\n",
    "    7. Selecci√≥n de caracter√≠sticas\n",
    "    \n",
    "    Par√°metros:\n",
    "    - X_train: DataFrame con caracter√≠sticas de entrenamiento\n",
    "    - X_test: DataFrame con caracter√≠sticas de test\n",
    "    - y_train: Series con etiquetas de entrenamiento\n",
    "    - y_test: Series con etiquetas de test\n",
    "    \n",
    "    Retorna:\n",
    "    - Diccionario con todos los resultados organizados por experimento\n",
    "    \"\"\"\n",
    "    print(\"üî¨ INICIANDO SUITE COMPLETA DE EXPERIMENTOS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Datos de entrenamiento: {X_train.shape}\")\n",
    "    print(f\"Datos de test: {X_test.shape}\")\n",
    "    print(f\"Distribuci√≥n de clases: {np.bincount(y_train)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Limpiar resultados globales\n",
    "    global_results.clear()\n",
    "    \n",
    "    # 1. Experimento Baseline\n",
    "    print(f\"\\n{'='*20} EXPERIMENTO 0: BASELINE {'='*20}\")\n",
    "    baseline_results = experiment_baseline(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 2. Experimento de √°rboles\n",
    "    print(f\"\\n{'='*20} EXPERIMENTO 1: √ÅRBOLES {'='*20}\")\n",
    "    tree_results = experiment_tree_based_features(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 3. Experimento de reducci√≥n de dimensionalidad\n",
    "    print(f\"\\n{'='*20} EXPERIMENTO 2: DIMENSIONALIDAD {'='*20}\")\n",
    "    dim_results = experiment_dimensionality_reduction(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 4. Experimento de interacciones\n",
    "    print(f\"\\n{'='*20} EXPERIMENTO 3: INTERACCIONES {'='*20}\")\n",
    "    interaction_results = experiment_feature_interactions(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 5. Experimento de clustering\n",
    "    print(f\"\\n{'='*20} EXPERIMENTO 4: CLUSTERING {'='*20}\")\n",
    "    clustering_results = experiment_clustering_features(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 6. Experimento de detecci√≥n de anomal√≠as\n",
    "    print(f\"\\n{'='*20} EXPERIMENTO 5: ANOMAL√çAS {'='*20}\")\n",
    "    anomaly_results = experiment_anomaly_features(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 7. Experimento de selecci√≥n de caracter√≠sticas\n",
    "    print(f\"\\n{'='*20} EXPERIMENTO 6: SELECCI√ìN {'='*20}\")\n",
    "    selection_results = experiment_feature_selection(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Mostrar resumen final\n",
    "    print_experiment_summary()\n",
    "    \n",
    "    # Guardar resultados en archivo JSON\n",
    "    output_path = \"feature_engineering_results.json\"\n",
    "    serializable_results = {\n",
    "        exp: {\n",
    "            method: {\n",
    "                k: float(v) if isinstance(v, (np.floating, np.float32, np.float64)) else v\n",
    "                for k, v in result.items() if k in ['accuracy', 'n_features']\n",
    "            }\n",
    "            for method, result in methods.items()\n",
    "        }\n",
    "        for exp, methods in global_results.items()\n",
    "    }\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    print(f\"\\nResultados guardados en {output_path}\")\n",
    "    \n",
    "    return global_results\n",
    "\n",
    "def get_best_features(X_train, X_test, y_train, y_test, method_name=None):\n",
    "    \"\"\"\n",
    "    Obtiene las mejores caracter√≠sticas transformadas bas√°ndose en los resultados\n",
    "    \n",
    "    Par√°metros:\n",
    "    - X_train, X_test, y_train, y_test: Datos originales\n",
    "    - method_name: Nombre espec√≠fico del m√©todo (opcional)\n",
    "    \n",
    "    Retorna:\n",
    "    - Tupla con (X_train_transformed, X_test_transformed) del mejor m√©todo\n",
    "    \"\"\"\n",
    "    if not global_results:\n",
    "        print(\"No hay resultados disponibles. Ejecuta los experimentos primero.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Encontrar el mejor m√©todo\n",
    "    best_accuracy = 0\n",
    "    best_method = None\n",
    "    best_experiment = None\n",
    "    \n",
    "    for exp_type, results in global_results.items():\n",
    "        for method, result in results.items():\n",
    "            if method_name and method != method_name:\n",
    "                continue\n",
    "            if result.get('accuracy', 0) > best_accuracy:\n",
    "                best_accuracy = result['accuracy']\n",
    "                best_method = method\n",
    "                best_experiment = exp_type\n",
    "    \n",
    "    if best_method is None:\n",
    "        print(\"No se encontr√≥ el m√©todo especificado.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Aplicando mejor m√©todo: {best_method} (accuracy: {best_accuracy:.4f})\")\n",
    "    \n",
    "    # Reejecutar el experimento espec√≠fico para obtener las caracter√≠sticas\n",
    "    if best_experiment == \"baseline\":\n",
    "        return X_train, X_test\n",
    "    elif best_experiment == \"tree_based\":\n",
    "        results = experiment_tree_based_features(X_train, X_test, y_train, y_test)\n",
    "    elif best_experiment == \"dimensionality_reduction\":\n",
    "        results = experiment_dimensionality_reduction(X_train, X_test, y_train, y_test)\n",
    "    elif best_experiment == \"feature_interactions\":\n",
    "        results = experiment_feature_interactions(X_train, X_test, y_train, y_test)\n",
    "    elif best_experiment == \"clustering\":\n",
    "        results = experiment_clustering_features(X_train, X_test, y_train, y_test)\n",
    "    elif best_experiment == \"anomaly_detection\":\n",
    "        results = experiment_anomaly_features(X_train, X_test, y_train, y_test)\n",
    "    elif best_experiment == \"feature_selection\":\n",
    "        results = experiment_feature_selection(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Nota: Esta funci√≥n devuelve None porque requerir√≠a reejecutar\n",
    "    # los experimentos para obtener las caracter√≠sticas transformadas\n",
    "    print(\"Para obtener las caracter√≠sticas transformadas, reejecutar el experimento espec√≠fico.\")\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fecc5e",
   "metadata": {},
   "source": [
    "### Ejecuci√≥n de los experimentos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "149e6e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Experimento Baseline: Caracter√≠sticas originales\n",
      "--------------------------------------------------\n",
      "  original_features: 0.8207 (features: 32)\n",
      "\n",
      "Baseline establecido con 32 caracter√≠sticas originales\n",
      "Accuracy baseline: 0.8207\n"
     ]
    }
   ],
   "source": [
    "# Experimento baseline\n",
    "baseline_results = experiment_baseline(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f32e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≥ Experimento 1: Caracter√≠sticas basadas en √°rboles\n",
      "--------------------------------------------------\n",
      "Generando caracter√≠sticas con XGBoost leaf indices...\n",
      "Generando caracter√≠sticas con Random Forest leaf indices...\n",
      "Combinando caracter√≠sticas de XGBoost y Random Forest...\n",
      "Calculando estad√≠sticas derivadas de hojas...\n",
      "  xgb_leaves: 0.9827 (features: 100)\n",
      "  rf_leaves: 0.9680 (features: 100)\n",
      "  combined_leaves: 0.9827 (features: 200)\n",
      "  leaf_distances: 0.7776 (features: 1)\n",
      "\n",
      "Mejor resultado en tree-based features:\n",
      "  xgb_leaves: 0.9827\n"
     ]
    }
   ],
   "source": [
    "# Experimento √°rboles\n",
    "tree_results = experiment_tree_based_features(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c12f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Experimento 2: Reducci√≥n de dimensionalidad\n",
      "--------------------------------------------------\n",
      "Aplicando PCA...\n",
      "  PCA: 32 ‚Üí 32 componentes\n",
      "  Varianza explicada: 1.000 (primeros 5 componentes)\n",
      "Aplicando TruncatedSVD...\n",
      "  SVD: 32 ‚Üí 31 componentes\n",
      "  Varianza explicada: 1.000 (primeros 5 componentes)\n",
      "Aplicando FastICA...\n",
      "  ICA: 32 ‚Üí 20 componentes independientes\n",
      "Combinando caracter√≠sticas originales con PCA...\n",
      "  Combinado: 32 + 32 = 64 caracter√≠sticas\n",
      "  pca_only: 0.8156 (features: 32)\n",
      "  svd_only: 0.8187 (features: 31)\n",
      "  ica_only: 0.7816 (features: 20)\n",
      "  original_plus_pca: 0.8492 (features: 64)\n",
      "\n",
      "Mejor resultado en reducci√≥n de dimensionalidad:\n",
      "  original_plus_pca: 0.8492\n"
     ]
    }
   ],
   "source": [
    "# Experimento dimensionalidad\n",
    "dim_results = experiment_dimensionality_reduction(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acd37430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Experimento 3: Interacciones entre caracter√≠sticas\n",
      "--------------------------------------------------\n",
      "Generando caracter√≠sticas polin√≥micas...\n",
      "  Polin√≥micas: 32 ‚Üí 528 caracter√≠sticas\n",
      "Generando interacciones manuales...\n",
      "  Interacciones manuales: 45 nuevas caracter√≠sticas\n",
      "Generando ratios entre caracter√≠sticas...\n",
      "  Ratios: 10 nuevas caracter√≠sticas\n",
      "  polynomial_features: 0.8883 (features: 528)\n",
      "  manual_interactions: 0.7918 (features: 45)\n",
      "  feature_ratios: 0.7816 (features: 10)\n",
      "\n",
      "Mejor resultado en interacciones de caracter√≠sticas:\n",
      "  polynomial_features: 0.8883\n"
     ]
    }
   ],
   "source": [
    "# Experimento interacciones\n",
    "interaction_results = experiment_feature_interactions(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e45f33c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Experimento 4: Caracter√≠sticas basadas en clustering\n",
      "--------------------------------------------------\n",
      "Aplicando K-Means clustering...\n",
      "  K-Means: 10 clusters creados\n",
      "  Distribuci√≥n de clusters en entrenamiento: [7139    1    1    1   27    1  682    2    7   11]\n",
      "Calculando distancias a centroides...\n",
      "  Distancias: 10 caracter√≠sticas de distancia\n",
      "Calculando centroides por clase...\n",
      "  Datos de fraude: 1752 muestras\n",
      "  Datos normales: 6120 muestras\n",
      "  Distancias por clase: 2 caracter√≠sticas (dist_fraud, dist_normal)\n",
      "  Distancia promedio a centroide fraude: 217445747.372\n",
      "  Distancia promedio a centroide normal: 313206800.770\n",
      "  kmeans_labels: 0.8603 (features: 1)\n",
      "  kmeans_distances: 0.7831 (features: 10)\n",
      "  class_distances: 0.7831 (features: 2)\n",
      "\n",
      "Mejor resultado en caracter√≠sticas de clustering:\n",
      "  kmeans_labels: 0.8603\n"
     ]
    }
   ],
   "source": [
    "# Experimento clustering\n",
    "clustering_results = experiment_clustering_features(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a20bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Experimento 5: Caracter√≠sticas de detecci√≥n de anomal√≠as\n",
      "--------------------------------------------------\n",
      "Aplicando Isolation Forest...\n",
      "  Isolation Forest: 1 caracter√≠stica de anomal√≠a\n",
      "  Score promedio entrenamiento: 0.152\n",
      "  Score promedio test: 0.153\n",
      "Calculando distancias de Mahalanobis...\n",
      "  Mahalanobis: 1 caracter√≠stica de distancia\n",
      "  Distancia promedio entrenamiento: 0.000\n",
      "  Distancia promedio test: 0.000\n",
      "Calculando estad√≠sticas locales (Z-scores)...\n",
      "  Estad√≠sticas locales: 32 caracter√≠sticas Z-score\n",
      "Creando estad√≠sticas agregadas...\n",
      "  Estad√≠sticas agregadas: 2 caracter√≠sticas\n",
      "  isolation_forest: 0.7760 (features: 1)\n",
      "  mahalanobis_distance: 0.7831 (features: 1)\n",
      "  local_statistics: Error - Input X contains infinity or a value too large for dtype('float64').\n",
      "  aggregated_stats: Error - Input X contains infinity or a value too large for dtype('float64').\n",
      "\n",
      "Mejor resultado en detecci√≥n de anomal√≠as:\n",
      "  mahalanobis_distance: 0.7831\n"
     ]
    }
   ],
   "source": [
    "# Experimento anomal√≠as\n",
    "anomaly_results = experiment_anomaly_features(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d52709dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Experimento 6: Selecci√≥n de caracter√≠sticas\n",
      "--------------------------------------------------\n",
      "Aplicando selecci√≥n por Mutual Information...\n",
      "  Mutual Information: 32 ‚Üí 20 caracter√≠sticas\n",
      "  Score promedio seleccionadas: 0.197\n",
      "  Top 3 caracter√≠sticas: ['Time Diff between first and last (Mins)', 'Received Tnx', 'Unique Received From Addresses']\n",
      "Aplicando selecci√≥n por F-score...\n",
      "  F-score: 32 ‚Üí 20 caracter√≠sticas\n",
      "  F-score promedio seleccionadas: 50.712\n",
      "  Top 3 caracter√≠sticas: ['Avg min between sent tnx', 'Avg min between received tnx', 'Time Diff between first and last (Mins)']\n",
      "Aplicando RFE con Random Forest...\n",
      "  RFE: 32 ‚Üí 15 caracter√≠sticas\n",
      "  Caracter√≠sticas seleccionadas: ['Avg min between received tnx', 'Time Diff between first and last (Mins)', 'Unique Received From Addresses']\n",
      "Aplicando SelectFromModel con XGBoost...\n",
      "  XGBoost: 32 ‚Üí 5 caracter√≠sticas\n",
      "  Umbral autom√°tico: 0.0312\n",
      "  Top 3 caracter√≠sticas: ['Time Diff between first and last (Mins)', 'total ether received', 'Total ERC20 tnxs']\n",
      "Analizando consenso entre m√©todos...\n",
      "  Caracter√≠sticas √∫nicas total: 29\n",
      "  Caracter√≠sticas en consenso: 3\n",
      "  Consenso: ['Total ERC20 tnxs', 'Time Diff between first and last (Mins)', 'total ether received']\n",
      "  mutual_info_selection: 0.7801 (features: 20)\n",
      "  f_score_selection: 0.8116 (features: 20)\n",
      "  rfe_selection: 0.7831 (features: 15)\n",
      "  xgb_selection: 0.7831 (features: 5)\n",
      "\n",
      "Mejor resultado en selecci√≥n de caracter√≠sticas:\n",
      "  f_score_selection: 0.8116\n"
     ]
    }
   ],
   "source": [
    "# Experimento selecci√≥n\n",
    "selection_results = experiment_feature_selection(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1534639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìã RESUMEN COMPLETO DE EXPERIMENTOS DE FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "üèÜ RANKING GENERAL (Top 15)\n",
      "--------------------------------------------------------------------------------\n",
      "Rank  Experiment           Method                    Accuracy   Features\n",
      "--------------------------------------------------------------------------------\n",
      "1     tree_based           xgb_leaves                0.9827    100     \n",
      "2     tree_based           combined_leaves           0.9827    200     \n",
      "3     tree_based           rf_leaves                 0.9680    100     \n",
      "4     feature_interactions polynomial_features       0.8883    528     \n",
      "5     clustering           kmeans_labels             0.8603    1       \n",
      "6     dimensionality_reduction original_plus_pca         0.8492    64      \n",
      "7     baseline             original_features         0.8207    32      \n",
      "8     dimensionality_reduction svd_only                  0.8187    31      \n",
      "9     dimensionality_reduction pca_only                  0.8156    32      \n",
      "10    feature_selection    f_score_selection         0.8116    20      \n",
      "11    feature_interactions manual_interactions       0.7918    45      \n",
      "12    clustering           kmeans_distances          0.7831    10      \n",
      "13    clustering           class_distances           0.7831    2       \n",
      "14    anomaly_detection    mahalanobis_distance      0.7831    1       \n",
      "15    feature_selection    rfe_selection             0.7831    15      \n",
      "\n",
      "üìä MEJORES M√âTODOS POR CATEGOR√çA\n",
      "--------------------------------------------------------------------------------\n",
      "tree_based          : xgb_leaves                (0.9827, 100 features)\n",
      "feature_interactions: polynomial_features       (0.8883, 528 features)\n",
      "clustering          : kmeans_labels             (0.8603, 1 features)\n",
      "dimensionality_reduction: original_plus_pca         (0.8492, 64 features)\n",
      "baseline            : original_features         (0.8207, 32 features)\n",
      "feature_selection   : f_score_selection         (0.8116, 20 features)\n",
      "anomaly_detection   : mahalanobis_distance      (0.7831, 1 features)\n",
      "\n",
      "üìà ESTAD√çSTICAS GENERALES\n",
      "--------------------------------------------------------------------------------\n",
      "Total de experimentos ejecutados: 21\n",
      "Accuracy promedio: 0.8287\n",
      "Accuracy m√°xima: 0.9827\n",
      "Accuracy m√≠nima: 0.7760\n",
      "Desviaci√≥n est√°ndar: 0.0677\n",
      "N√∫mero promedio de caracter√≠sticas: 59.0\n",
      "Rango de caracter√≠sticas: 1 - 528\n",
      "\n",
      "‚ö° AN√ÅLISIS DE EFICIENCIA\n",
      "--------------------------------------------------------------------------------\n",
      "M√©todos eficientes (alta accuracy, pocas caracter√≠sticas):\n",
      "  kmeans_labels            : 0.8603 (1 features, score: 0.4302)\n",
      "\n",
      "üí° RECOMENDACIONES\n",
      "--------------------------------------------------------------------------------\n",
      "ü•á Mejor m√©todo general: xgb_leaves\n",
      "   Categor√≠a: tree_based\n",
      "   Accuracy: 0.9827\n",
      "   Caracter√≠sticas: 100\n",
      "üöÄ Mejor m√©todo eficiente: kmeans_labels\n",
      "   Accuracy: 0.8603\n",
      "   Caracter√≠sticas: 1\n"
     ]
    }
   ],
   "source": [
    "# Resumen final\n",
    "print_experiment_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0121f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ INICIANDO SUITE COMPLETA DE EXPERIMENTOS\n",
      "================================================================================\n",
      "Datos de entrenamiento: (7872, 32)\n",
      "Datos de test: (1969, 32)\n",
      "Distribuci√≥n de clases: [6120 1752]\n",
      "================================================================================\n",
      "\n",
      "==================== EXPERIMENTO 0: BASELINE ====================\n",
      "üìä Experimento Baseline: Caracter√≠sticas originales\n",
      "--------------------------------------------------\n",
      "  original_features: 0.8207 (features: 32)\n",
      "\n",
      "Baseline establecido con 32 caracter√≠sticas originales\n",
      "Accuracy baseline: 0.8207\n",
      "\n",
      "==================== EXPERIMENTO 1: √ÅRBOLES ====================\n",
      "üå≥ Experimento 1: Caracter√≠sticas basadas en √°rboles\n",
      "--------------------------------------------------\n",
      "Generando caracter√≠sticas con XGBoost leaf indices...\n",
      "Generando caracter√≠sticas con Random Forest leaf indices...\n",
      "Combinando caracter√≠sticas de XGBoost y Random Forest...\n",
      "Calculando estad√≠sticas derivadas de hojas...\n",
      "  xgb_leaves: 0.9827 (features: 100)\n",
      "  rf_leaves: 0.9680 (features: 100)\n",
      "  combined_leaves: 0.9827 (features: 200)\n",
      "  leaf_distances: 0.7776 (features: 1)\n",
      "\n",
      "Mejor resultado en tree-based features:\n",
      "  xgb_leaves: 0.9827\n",
      "\n",
      "==================== EXPERIMENTO 2: DIMENSIONALIDAD ====================\n",
      "üìâ Experimento 2: Reducci√≥n de dimensionalidad\n",
      "--------------------------------------------------\n",
      "Aplicando PCA...\n",
      "  PCA: 32 ‚Üí 32 componentes\n",
      "  Varianza explicada: 1.000 (primeros 5 componentes)\n",
      "Aplicando TruncatedSVD...\n",
      "  SVD: 32 ‚Üí 31 componentes\n",
      "  Varianza explicada: 1.000 (primeros 5 componentes)\n",
      "Aplicando FastICA...\n",
      "  ICA: 32 ‚Üí 20 componentes independientes\n",
      "Combinando caracter√≠sticas originales con PCA...\n",
      "  Combinado: 32 + 32 = 64 caracter√≠sticas\n",
      "  pca_only: 0.8156 (features: 32)\n",
      "  svd_only: 0.8187 (features: 31)\n",
      "  ica_only: 0.7816 (features: 20)\n",
      "  original_plus_pca: 0.8492 (features: 64)\n",
      "\n",
      "Mejor resultado en reducci√≥n de dimensionalidad:\n",
      "  original_plus_pca: 0.8492\n",
      "\n",
      "==================== EXPERIMENTO 3: INTERACCIONES ====================\n",
      "üîÑ Experimento 3: Interacciones entre caracter√≠sticas\n",
      "--------------------------------------------------\n",
      "Generando caracter√≠sticas polin√≥micas...\n",
      "  Polin√≥micas: 32 ‚Üí 528 caracter√≠sticas\n",
      "Generando interacciones manuales...\n",
      "  Interacciones manuales: 45 nuevas caracter√≠sticas\n",
      "Generando ratios entre caracter√≠sticas...\n",
      "  Ratios: 10 nuevas caracter√≠sticas\n",
      "  polynomial_features: 0.8883 (features: 528)\n",
      "  manual_interactions: 0.7918 (features: 45)\n",
      "  feature_ratios: 0.7816 (features: 10)\n",
      "\n",
      "Mejor resultado en interacciones de caracter√≠sticas:\n",
      "  polynomial_features: 0.8883\n",
      "\n",
      "==================== EXPERIMENTO 4: CLUSTERING ====================\n",
      "üéØ Experimento 4: Caracter√≠sticas basadas en clustering\n",
      "--------------------------------------------------\n",
      "Aplicando K-Means clustering...\n",
      "  K-Means: 10 clusters creados\n",
      "  Distribuci√≥n de clusters en entrenamiento: [7139    1    1    1   27    1  682    2    7   11]\n",
      "Calculando distancias a centroides...\n",
      "  Distancias: 10 caracter√≠sticas de distancia\n",
      "Calculando centroides por clase...\n",
      "  Datos de fraude: 1752 muestras\n",
      "  Datos normales: 6120 muestras\n",
      "  Distancias por clase: 2 caracter√≠sticas (dist_fraud, dist_normal)\n",
      "  Distancia promedio a centroide fraude: 217445747.372\n",
      "  Distancia promedio a centroide normal: 313206800.770\n",
      "  kmeans_labels: 0.8603 (features: 1)\n",
      "  kmeans_distances: 0.7831 (features: 10)\n",
      "  class_distances: 0.7831 (features: 2)\n",
      "\n",
      "Mejor resultado en caracter√≠sticas de clustering:\n",
      "  kmeans_labels: 0.8603\n",
      "\n",
      "==================== EXPERIMENTO 5: ANOMAL√çAS ====================\n",
      "üö® Experimento 5: Caracter√≠sticas de detecci√≥n de anomal√≠as\n",
      "--------------------------------------------------\n",
      "Aplicando Isolation Forest...\n",
      "  Isolation Forest: 1 caracter√≠stica de anomal√≠a\n",
      "  Score promedio entrenamiento: 0.152\n",
      "  Score promedio test: 0.153\n",
      "Calculando distancias de Mahalanobis...\n",
      "  Mahalanobis: 1 caracter√≠stica de distancia\n",
      "  Distancia promedio entrenamiento: 0.000\n",
      "  Distancia promedio test: 0.000\n",
      "Calculando estad√≠sticas locales (Z-scores)...\n",
      "  Estad√≠sticas locales: 32 caracter√≠sticas Z-score\n",
      "Creando estad√≠sticas agregadas...\n",
      "  Estad√≠sticas agregadas: 2 caracter√≠sticas\n",
      "  isolation_forest: 0.7760 (features: 1)\n",
      "  mahalanobis_distance: 0.7831 (features: 1)\n",
      "  local_statistics: Error - Input X contains infinity or a value too large for dtype('float64').\n",
      "  aggregated_stats: Error - Input X contains infinity or a value too large for dtype('float64').\n",
      "\n",
      "Mejor resultado en detecci√≥n de anomal√≠as:\n",
      "  mahalanobis_distance: 0.7831\n",
      "\n",
      "==================== EXPERIMENTO 6: SELECCI√ìN ====================\n",
      "üéØ Experimento 6: Selecci√≥n de caracter√≠sticas\n",
      "--------------------------------------------------\n",
      "Aplicando selecci√≥n por Mutual Information...\n",
      "  Mutual Information: 32 ‚Üí 20 caracter√≠sticas\n",
      "  Score promedio seleccionadas: 0.199\n",
      "  Top 3 caracter√≠sticas: ['Time Diff between first and last (Mins)', 'Received Tnx', 'Unique Received From Addresses']\n",
      "Aplicando selecci√≥n por F-score...\n",
      "  F-score: 32 ‚Üí 20 caracter√≠sticas\n",
      "  F-score promedio seleccionadas: 50.712\n",
      "  Top 3 caracter√≠sticas: ['Avg min between sent tnx', 'Avg min between received tnx', 'Time Diff between first and last (Mins)']\n",
      "Aplicando RFE con Random Forest...\n",
      "  RFE: 32 ‚Üí 15 caracter√≠sticas\n",
      "  Caracter√≠sticas seleccionadas: ['Avg min between received tnx', 'Time Diff between first and last (Mins)', 'Unique Received From Addresses']\n",
      "Aplicando SelectFromModel con XGBoost...\n",
      "  XGBoost: 32 ‚Üí 5 caracter√≠sticas\n",
      "  Umbral autom√°tico: 0.0312\n",
      "  Top 3 caracter√≠sticas: ['Time Diff between first and last (Mins)', 'total ether received', 'Total ERC20 tnxs']\n",
      "Analizando consenso entre m√©todos...\n",
      "  Caracter√≠sticas √∫nicas total: 29\n",
      "  Caracter√≠sticas en consenso: 3\n",
      "  Consenso: ['Total ERC20 tnxs', 'Time Diff between first and last (Mins)', 'total ether received']\n",
      "  mutual_info_selection: 0.7801 (features: 20)\n",
      "  f_score_selection: 0.8116 (features: 20)\n",
      "  rfe_selection: 0.7831 (features: 15)\n",
      "  xgb_selection: 0.7831 (features: 5)\n",
      "\n",
      "Mejor resultado en selecci√≥n de caracter√≠sticas:\n",
      "  f_score_selection: 0.8116\n",
      "\n",
      "================================================================================\n",
      "üìã RESUMEN COMPLETO DE EXPERIMENTOS DE FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "üèÜ RANKING GENERAL (Top 15)\n",
      "--------------------------------------------------------------------------------\n",
      "Rank  Experiment           Method                    Accuracy   Features\n",
      "--------------------------------------------------------------------------------\n",
      "1     tree_based           xgb_leaves                0.9827    100     \n",
      "2     tree_based           combined_leaves           0.9827    200     \n",
      "3     tree_based           rf_leaves                 0.9680    100     \n",
      "4     feature_interactions polynomial_features       0.8883    528     \n",
      "5     clustering           kmeans_labels             0.8603    1       \n",
      "6     dimensionality_reduction original_plus_pca         0.8492    64      \n",
      "7     baseline             original_features         0.8207    32      \n",
      "8     dimensionality_reduction svd_only                  0.8187    31      \n",
      "9     dimensionality_reduction pca_only                  0.8156    32      \n",
      "10    feature_selection    f_score_selection         0.8116    20      \n",
      "11    feature_interactions manual_interactions       0.7918    45      \n",
      "12    clustering           kmeans_distances          0.7831    10      \n",
      "13    clustering           class_distances           0.7831    2       \n",
      "14    anomaly_detection    mahalanobis_distance      0.7831    1       \n",
      "15    feature_selection    rfe_selection             0.7831    15      \n",
      "\n",
      "üìä MEJORES M√âTODOS POR CATEGOR√çA\n",
      "--------------------------------------------------------------------------------\n",
      "tree_based          : xgb_leaves                (0.9827, 100 features)\n",
      "feature_interactions: polynomial_features       (0.8883, 528 features)\n",
      "clustering          : kmeans_labels             (0.8603, 1 features)\n",
      "dimensionality_reduction: original_plus_pca         (0.8492, 64 features)\n",
      "baseline            : original_features         (0.8207, 32 features)\n",
      "feature_selection   : f_score_selection         (0.8116, 20 features)\n",
      "anomaly_detection   : mahalanobis_distance      (0.7831, 1 features)\n",
      "\n",
      "üìà ESTAD√çSTICAS GENERALES\n",
      "--------------------------------------------------------------------------------\n",
      "Total de experimentos ejecutados: 21\n",
      "Accuracy promedio: 0.8287\n",
      "Accuracy m√°xima: 0.9827\n",
      "Accuracy m√≠nima: 0.7760\n",
      "Desviaci√≥n est√°ndar: 0.0677\n",
      "N√∫mero promedio de caracter√≠sticas: 59.0\n",
      "Rango de caracter√≠sticas: 1 - 528\n",
      "\n",
      "‚ö° AN√ÅLISIS DE EFICIENCIA\n",
      "--------------------------------------------------------------------------------\n",
      "M√©todos eficientes (alta accuracy, pocas caracter√≠sticas):\n",
      "  kmeans_labels            : 0.8603 (1 features, score: 0.4302)\n",
      "\n",
      "üí° RECOMENDACIONES\n",
      "--------------------------------------------------------------------------------\n",
      "ü•á Mejor m√©todo general: xgb_leaves\n",
      "   Categor√≠a: tree_based\n",
      "   Accuracy: 0.9827\n",
      "   Caracter√≠sticas: 100\n",
      "üöÄ Mejor m√©todo eficiente: kmeans_labels\n",
      "   Accuracy: 0.8603\n",
      "   Caracter√≠sticas: 1\n",
      "\n",
      "Resultados guardados en feature_engineering_results.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ejecutar todo de una vez \n",
    "all_results = run_all_experiments(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
